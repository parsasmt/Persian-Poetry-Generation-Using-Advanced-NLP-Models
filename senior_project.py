# -*- coding: utf-8 -*-
"""Senior_Project.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1ncnd7hkPGoAMShV1RrKbano1203jgcYa
"""

import requests
from bs4 import BeautifulSoup
import json
import time

"""#Data Scraping

##حافظ
"""

import requests
from bs4 import BeautifulSoup
import json
import time  # Import the time module for delays

# Base URL for the pages
base_url = 'https://ganjoor.net/hafez/ghazal/sh{}'

# Initialize an empty list to store all poems data
all_poems_data = []

# Loop through the first 10 pages
for n in range(1, 496):  # Pages 1 to 10

    if 351 <= n <= 356:
        continue  # Skip this iteration and move to the next page
    # Create the URL for the current page
    url = base_url.format(n)

    # Extract the poem number from the URL
    poem_number = str(n)  # Page number corresponds to the loop variable n
    print(n)

    # Send a GET request to the webpage
    response = requests.get(url)

    # Parse the page content using BeautifulSoup
    soup = BeautifulSoup(response.content, 'html.parser')

    # Initialize an empty list to store the poems and explanations for this page
    poems_data = []

    # Find all the poem blocks (divs with class 'b')
    poem_blocks = soup.find_all('div', class_='b')

    # Find the blockquotes and their corresponding notices
    summary_section = soup.find('div', class_='poem-info')

    # Inspect the structure of blockquotes and explanations in the summary section
    explanation_dict = {}
    if summary_section:
        blockquotes = summary_section.find_all('blockquote')

        for blockquote in blockquotes:
            a_tag = blockquote.find('a')
            if a_tag:
                poem_id = a_tag.get('href').replace('#', '')  # Remove "#" from poem ID

                # Find the next 'div' with class 'notice' (which contains the explanation)
                notice_div = blockquote.find_next('div', class_='notice')
                if notice_div:
                    explanation_text = notice_div.text.strip().replace('\n', ' ')  # Replace newlines with spaces
                else:
                    explanation_text = "does not exist"

                explanation_dict[poem_id] = explanation_text

    # Loop through each poem block
    for block in poem_blocks:
        # Extract poem ID (for linking with explanation)
        poem_id = block.get('id')

        # Extract poem verses
        verse_1 = block.find('div', class_='m1').text.strip()
        verse_2 = block.find('div', class_='m2').text.strip()

        # Combine the verses into a poem with " | " separator
        poem_text = f'{verse_1} | {verse_2}'

        # Check if there's an explanation for this poem
        explanation = explanation_dict.get(poem_id, "does not exist")

        # Append the poem number first, followed by the poem and explanation
        poems_data.append({
            'poem number': poem_number,  # Add the poem number first
            'poem': poem_text,
            'explanation': explanation
        })

    # Add the poems data from this page to the all_poems_data list
    all_poems_data.extend(poems_data)

    # Delay before the next request
    time.sleep(2)  # Wait for 2 seconds (you can adjust the duration)

# Convert the list of dictionaries to JSON format
output_json = json.dumps(all_poems_data, ensure_ascii=False, indent=4)

print(output_json)

with open('hafez_ghazal_poems_data.json', 'w', encoding='utf-8') as f:
    f.write(output_json)

"""##سعدی"""

import requests
from bs4 import BeautifulSoup
import json
import time  # Import the time module for delays

# Base URL for the pages
base_url = 'https://ganjoor.net/saadi/divan/ghazals/sh{}'

# Initialize an empty list to store all poems data
all_poems_data = []

# Loop through the first 10 pages
for n in range(1, 496):  # Pages 1 to 10

    if 351 <= n <= 356:
        continue  # Skip this iteration and move to the next page
    # Create the URL for the current page
    url = base_url.format(n)

    # Extract the poem number from the URL
    poem_number = str(n)  # Page number corresponds to the loop variable n
    print(n)

    # Send a GET request to the webpage
    response = requests.get(url)

    # Parse the page content using BeautifulSoup
    soup = BeautifulSoup(response.content, 'html.parser')

    # Initialize an empty list to store the poems and explanations for this page
    poems_data = []

    # Find all the poem blocks (divs with class 'b')
    poem_blocks = soup.find_all('div', class_='b')

    # Find the blockquotes and their corresponding notices
    summary_section = soup.find('div', class_='poem-info')

    # Inspect the structure of blockquotes and explanations in the summary section
    explanation_dict = {}
    if summary_section:
        blockquotes = summary_section.find_all('blockquote')

        for blockquote in blockquotes:
            a_tag = blockquote.find('a')
            if a_tag:
                poem_id = a_tag.get('href').replace('#', '')  # Remove "#" from poem ID

                # Find the next 'div' with class 'notice' (which contains the explanation)
                notice_div = blockquote.find_next('div', class_='notice')
                if notice_div:
                    explanation_text = notice_div.text.strip().replace('\n', ' ')  # Replace newlines with spaces
                else:
                    explanation_text = "does not exist"

                explanation_dict[poem_id] = explanation_text

    # Loop through each poem block
    for block in poem_blocks:
        # Extract poem ID (for linking with explanation)
        poem_id = block.get('id')

        # Extract poem verses
        verse_1 = block.find('div', class_='m1').text.strip()
        verse_2 = block.find('div', class_='m2').text.strip()

        # Combine the verses into a poem with " | " separator
        poem_text = f'{verse_1} | {verse_2}'

        # Check if there's an explanation for this poem
        explanation = explanation_dict.get(poem_id, "does not exist")

        # Append the poem number first, followed by the poem and explanation
        poems_data.append({
            'poem number': poem_number,  # Add the poem number first
            'poem': poem_text,
            'explanation': explanation
        })

    # Add the poems data from this page to the all_poems_data list
    all_poems_data.extend(poems_data)

    # Delay before the next request
    time.sleep(2)  # Wait for 2 seconds (you can adjust the duration)

# Convert the list of dictionaries to JSON format
output_json = json.dumps(all_poems_data, ensure_ascii=False, indent=4)

print(output_json)

with open('saadi_ghazal_poems_data.json', 'w', encoding='utf-8') as f:
    f.write(output_json)

"""##مولانا"""

import requests
from bs4 import BeautifulSoup
import json
import time  # Import the time module for delays

# Base URL for the pages
base_url = 'https://ganjoor.net/moulavi/shams/ghazalsh/sh{}'

# Initialize an empty list to store all poems data
all_poems_data = []

# Loop through the first 10 pages
for n in range(1, 496):  # Pages 1 to 10

    if 351 <= n <= 356:
        continue  # Skip this iteration and move to the next page
    # Create the URL for the current page
    url = base_url.format(n)

    # Extract the poem number from the URL
    poem_number = str(n)  # Page number corresponds to the loop variable n
    print(n)

    # Send a GET request to the webpage
    response = requests.get(url)

    # Parse the page content using BeautifulSoup
    soup = BeautifulSoup(response.content, 'html.parser')

    # Initialize an empty list to store the poems and explanations for this page
    poems_data = []

    # Find all the poem blocks (divs with class 'b')
    poem_blocks = soup.find_all('div', class_='b')

    # Find the blockquotes and their corresponding notices
    summary_section = soup.find('div', class_='poem-info')

    # Inspect the structure of blockquotes and explanations in the summary section
    explanation_dict = {}
    if summary_section:
        blockquotes = summary_section.find_all('blockquote')

        for blockquote in blockquotes:
            a_tag = blockquote.find('a')
            if a_tag:
                poem_id = a_tag.get('href').replace('#', '')  # Remove "#" from poem ID

                # Find the next 'div' with class 'notice' (which contains the explanation)
                notice_div = blockquote.find_next('div', class_='notice')
                if notice_div:
                    explanation_text = notice_div.text.strip().replace('\n', ' ')  # Replace newlines with spaces
                else:
                    explanation_text = "does not exist"

                explanation_dict[poem_id] = explanation_text

    # Loop through each poem block
    for block in poem_blocks:
        # Extract poem ID (for linking with explanation)
        poem_id = block.get('id')

        # Extract poem verses
        verse_1 = block.find('div', class_='m1').text.strip()
        verse_2 = block.find('div', class_='m2').text.strip()

        # Combine the verses into a poem with " | " separator
        poem_text = f'{verse_1} | {verse_2}'

        # Check if there's an explanation for this poem
        explanation = explanation_dict.get(poem_id, "does not exist")

        # Append the poem number first, followed by the poem and explanation
        poems_data.append({
            'poem number': poem_number,  # Add the poem number first
            'poem': poem_text,
            'explanation': explanation
        })

    # Add the poems data from this page to the all_poems_data list
    all_poems_data.extend(poems_data)

    # Delay before the next request
    time.sleep(2)  # Wait for 2 seconds (you can adjust the duration)

# Convert the list of dictionaries to JSON format
output_json = json.dumps(all_poems_data, ensure_ascii=False, indent=4)

print(output_json)

with open('molana_ghazal_poems_data.json', 'w', encoding='utf-8') as f:
    f.write(output_json)

"""#load data

"""

import json

file_path = '/content/hafez_ghazal_poems_data.json'

# Attempt to open the JSON file with a specific encoding
with open(file_path, 'r', encoding='utf-8', errors='ignore') as file:
    data = json.load(file)  # Load JSON data

data[0]

"""#deleting stop words

"""

import json

# Sample stop words list
stop_words = set([
    "و", "از", "به", "در", "که", "این", "است", "با", "برای","را", "نمی",
    "تا", "درست", "هم", "بود", "هر", "یک", "خود", "او","تو","من","دل","می","ای", "ما","کردم","یا","چه","چرا","حافظ", "شما",
    "آن", "علیه", "همچنان", "بر", "پس", "اگر", "هرگز"
])

# Sample dataset


# Function to remove stop words from a poem
def remove_stop_words(poem):
    words = poem.split()
    filtered_words = [word for word in words if word not in stop_words]
    return ' '.join(filtered_words)

# Process the dataset
for entry in data:
    entry['poem'] = remove_stop_words(entry['poem'])

# Print the modified dataset
new=json.dumps(data, ensure_ascii=False, indent=4)



# print(data[:20])

with open('hafez_ghazal_poems_no_stop_data.json', 'w', encoding='utf-8') as f:
    f.write(new)

path="/content/hafez_ghazal_poems_no_stop_data.json"

with open(path, 'r', encoding='utf-8', errors='ignore') as file:
    new_json = json.load(file)  # Load JSON data
print(new_json[0])

"""#topic modeling"""

!pip install bertopic

# Define the file path
# file_path = '/content/poems/hafez_ghazal_poems_data.json'

# # Attempt to open the JSON file with a specific encoding
# with open(file_path, 'r', encoding='utf-8', errors='ignore') as file:
#     data = json.load(file)  # Load JSON data
#     # data = data['explanation'].replace('\u200c', '')
# # print(len(data))  # Print the data or process it as needed
# topic = [item['poem'] for item in data ]
# topic

from bertopic import BERTopic
from collections import defaultdict

# Initialize BERTopic model
topic_model = BERTopic(language="multilingual")

# Sample poem data
poems = new_json

# Group poems by poet number and treat each poem as an individual document
grouped_poems = defaultdict(list)
for poem in poems:
    grouped_poems[poem["poem number"]].append(poem["poem"])

# Flatten poems for topic modeling and keep track of mappings
all_poems = [poem["poem"] for poem in poems]
topics, probs = topic_model.fit_transform(all_poems)

# Map each poem to its topics and add keywords to the explanation
for i, poem in enumerate(poems):
    topic_keywords = topic_model.get_topic(topics[i])
    # Get the top 3 keywords for the topic
    keywords_text = ', '.join([kw[0] for kw in topic_keywords[:3]]) if topic_keywords else "No keywords found"
    poem["explanation"] = keywords_text

# Display updated poems with keyword-based explanations
for poem in poems:
    print(f"Poem Number: {poem['poem number']}, Poem: {poem['poem']}, Explanation (Keywords): {poem['explanation']}")

"""#new format"""

import json
from collections import defaultdict

# Define stop words
stop_words = set([
    "و", "از", "به", "در", "که", "این", "است", "با", "برای", "را", "نمی",
    "تا", "درست", "هم", "بود", "هر", "یک", "خود", "او", "تو", "من", "دل", "می", "ای", "ما", "کردم", "یا", "چه", "چرا", "حافظ", "شما",
    "آن", "علیه", "همچنان", "بر", "پس", "اگر", "هرگز",'دارد', 'دوست', 'رفت', 'سر', 'شد', 'می', 'نبود','آمده', 'ای', 'باد', 'جان', 'خواهد', 'خوش', 'دانست', 'زده', 'می', 'نیز', 'چون', 'کنید', 'گر','نرود', 'نمی', 'نیست', 'چو', 'کرد', 'کنند', 'گفت', 'گیرد'
])

# Load the original data
with open('/content/hafez_ghazal_poems_data.json', 'r', encoding='utf-8') as file:
    data = json.load(file)

# Organize poems by their poem number
grouped_poems = defaultdict(list)
for item in data:
    poem_number = item['poem number']
    poem_text = item['poem']
    grouped_poems[poem_number].append(poem_text)

# Function to remove stop words from a poem
def remove_stop_words(poem):
    words = poem.split()
    filtered_words = [word for word in words if word not in stop_words]
    return ' '.join(filtered_words)

# Process the dataset and remove stop words from each poem
transformed_data = [
    {
        'poem number': f'poem number {poem_number}',
        'poems': '\n '.join(remove_stop_words(poem) for poem in poems)  # Concatenate the poems into a single string
    }
    for poem_number, poems in grouped_poems.items()
]

# Save the modified data to a new JSON file
with open('/content/transformed_poems_no_stopwords.json', 'w', encoding='utf-8') as file:
    json.dump(transformed_data, file, ensure_ascii=False, indent=4)

print("Transformation complete! Check 'transformed_poems_no_stopwords.json' for the result.")

"""# Trying different topic modeling methods

##bert
"""

import json
from bertopic import BERTopic
from collections import defaultdict

# Load the cleaned data from your JSON file
with open('/content/transformed_poems_no_stopwords.json', 'r', encoding='utf-8') as file:
    poems = json.load(file)  # Load your cleaned dataset

# Initialize BERTopic model
topic_model = BERTopic(language="multilingual", min_topic_size=10, n_gram_range=(1, 2))

# Group poems by poem number and treat each poem as an individual document
grouped_poems = defaultdict(list)
for poem in poems:
    grouped_poems[poem["poem number"]].append(poem["poems"])

# Flatten poems for topic modeling
all_poems = [poem["poems"] for poem in poems]
topics, probs = topic_model.fit_transform(all_poems)

# Prepare the output data
output_data = []
for i, poem in enumerate(poems):
    topic_keywords = topic_model.get_topic(topics[i])
    # Get the top 3 keywords for the topic
    keywords_list = [kw[0] for kw in topic_keywords[:6]] if topic_keywords else []
    output_data.append({
        'poem number': poem['poem number'],
        'poem': poem['poems'],
        'topic': topics[i],  # Store the topic number
        'keywords': keywords_list  # Store the list of keywords
    })

# Save the output data to a new JSON file
with open('/content/transformed_poems_with_topics.json', 'w', encoding='utf-8') as file:
    json.dump(output_data, file, ensure_ascii=False, indent=4)

print("Topic modeling complete! Check 'transformed_poems_with_topics.json' for the result.")

unique_keywords_set = set()

for poem in output_data:
    keywords = poem['keywords']  # Get the keywords for each poem
    unique_keywords_set.update(keywords)  # Add keywords to the set for uniqueness

# Convert the set of unique keywords to a sorted list for better readability
unique_keywords_list = sorted(list(unique_keywords_set))

# Print unique keywords
print("Unique Keywords:")
print(unique_keywords_list)

from sentence_transformers import SentenceTransformer
from bertopic import BERTopic
from sklearn.feature_extraction.text import CountVectorizer
import json

# Load your data
with open('/content/transformed_poems_no_stopwords.json', 'r', encoding='utf-8') as file:
    poems = json.load(file)

# Define your custom Persian stop words
persian_stop_words = set([
    "و", "از", "به", "در", "که", "این", "است", "با", "برای", "را", "نمی",
    "تا", "درست", "هم", "بود", "هر", "یک", "خود", "او", "تو", "من", "دل",
    "می", "ای", "ما", "کردم", "یا", "چه", "چرا", "حافظ", "شما", "آن",
    "علیه", "همچنان", "بر", "پس", "اگر", "هرگز"
])

# Use a sentence transformer model for Persian
embedding_model = SentenceTransformer("stsb-xlm-r-bert-base")

# Initialize BERTopic with custom vectorizer
vectorizer_model = CountVectorizer(ngram_range=(1, 2), stop_words=persian_stop_words)
topic_model = BERTopic(embedding_model=embedding_model, vectorizer_model=vectorizer_model, language="multilingual")

# Prepare poems for modeling
all_poems = [poem["poems"] for poem in poems]
topics, probs = topic_model.fit_transform(all_poems)

# Review topics and keywords
topic_info = topic_model.get_topic_info()
print(topic_info)

pip install transformers

"""##parsbert

"""

import json
from bertopic import BERTopic
from transformers import AutoTokenizer, AutoModel
import torch
import numpy as np
from collections import defaultdict

# Load ParsBERT model and tokenizer
model_name = "HooshvareLab/bert-base-parsbert-uncased"
tokenizer = AutoTokenizer.from_pretrained(model_name)
parsbert_model = AutoModel.from_pretrained(model_name)

# Load the cleaned data from your JSON file
with open('/content/transformed_poems_no_stopwords.json', 'r', encoding='utf-8') as file:
    poems = json.load(file)

# Define a function to generate ParsBERT embeddings
def get_parsbert_embeddings(text_list):
    embeddings = []
    for text in text_list:
        inputs = tokenizer(text, return_tensors="pt", padding=True, truncation=True)
        with torch.no_grad():
            outputs = parsbert_model(**inputs)
        # Pool the embeddings to get a single vector for each text
        embeddings.append(outputs.last_hidden_state.mean(dim=1).squeeze().numpy())
    return np.array(embeddings)  # Convert to a NumPy array

# Generate ParsBERT embeddings for all poems
all_poems = [poem["poems"] for poem in poems]
parsbert_embeddings = get_parsbert_embeddings(all_poems)

# Initialize BERTopic model with custom embeddings
topic_model = BERTopic(language="multilingual", min_topic_size=10, n_gram_range=(1, 2))

# Fit the BERTopic model on ParsBERT embeddings
topics, probs = topic_model.fit_transform(all_poems, embeddings=parsbert_embeddings)

# Prepare the output data
output_data = []
for i, poem in enumerate(poems):
    topic_keywords = topic_model.get_topic(topics[i])
    # Get the top 6 keywords for the topic
    keywords_list = [kw[0] for kw in topic_keywords[:3]] if topic_keywords else []
    output_data.append({
        'poem number': poem['poem number'],
        'poem': poem['poems'],
        'topic': topics[i],
        'keywords': keywords_list
    })

# Save the output data to a new JSON file
with open('/content/transformed_poems_with_parsbert_topics.json', 'w', encoding='utf-8') as file:
    json.dump(output_data, file, ensure_ascii=False, indent=4)

print("Topic modeling with ParsBERT embeddings complete! Check 'transformed_poems_with_parsbert_topics.json' for the result.")

unique_keywords_set = set()

for poem in output_data:
    keywords = poem['keywords']  # Get the keywords for each poem
    unique_keywords_set.update(keywords)  # Add keywords to the set for uniqueness

# Convert the set of unique keywords to a sorted list for better readability
unique_keywords_list = sorted(list(unique_keywords_set))

# Print unique keywords
print("Unique Keywords:")
print(unique_keywords_list)

"""##parsbert with LDA"""

import json
from gensim import corpora
from gensim.models import LdaModel
from transformers import AutoTokenizer, AutoModel
import torch
import numpy as np
from sklearn.metrics.pairwise import cosine_similarity
from collections import defaultdict

# Load ParsBERT model and tokenizer
model_name = "HooshvareLab/bert-base-parsbert-uncased"
tokenizer = AutoTokenizer.from_pretrained(model_name)
parsbert_model = AutoModel.from_pretrained(model_name)

# Load the cleaned data from your JSON file
with open('/content/transformed_poems_no_stopwords.json', 'r', encoding='utf-8') as file:
    poems = json.load(file)

# Prepare data for LDA
all_poems = [poem["poems"] for poem in poems]
tokenized_poems = [poem.split() for poem in all_poems]
dictionary = corpora.Dictionary(tokenized_poems)
corpus = [dictionary.doc2bow(poem) for poem in tokenized_poems]

# Train LDA model
lda_model = LdaModel(corpus=corpus, id2word=dictionary, num_topics=10, random_state=42, passes=10, alpha='auto')

# Get dominant LDA topics for each poem
lda_topics = []
for doc in corpus:
    topic_probs = lda_model.get_document_topics(doc)
    dominant_topic = max(topic_probs, key=lambda x: x[1])[0]
    lda_topics.append(dominant_topic)

# Generate ParsBERT embeddings for each poem
def get_parsbert_embeddings(text_list):
    embeddings = []
    for text in text_list:
        inputs = tokenizer(text, return_tensors="pt", padding=True, truncation=True)
        with torch.no_grad():
            outputs = parsbert_model(**inputs)
        embeddings.append(outputs.last_hidden_state.mean(dim=1).squeeze().numpy())
    return np.array(embeddings)

parsbert_embeddings = get_parsbert_embeddings(all_poems)

# Compute keywords for each LDA topic using ParsBERT embeddings
output_data = []
topic_keywords = defaultdict(list)

# Group poems by LDA topics
for idx, topic in enumerate(lda_topics):
    topic_keywords[topic].append(parsbert_embeddings[idx])

# Summarize each topic's keywords by finding the most similar terms within each group
for topic, embeddings in topic_keywords.items():
    if len(embeddings) > 1:
        # Compute average embedding for the topic
        avg_embedding = np.mean(embeddings, axis=0).reshape(1, -1)
        # Get keywords by finding words closest to the average embedding in each poem
        keywords = []
        for idx, poem in enumerate(all_poems):
            poem_embedding = parsbert_embeddings[idx].reshape(1, -1)
            similarity = cosine_similarity(avg_embedding, poem_embedding)[0][0]
            if similarity > 0.8:  # Adjust threshold based on experimentation
                keywords.append(poem)
        topic_keywords[topic] = keywords[:6]  # Limit to top 6 keywords

# Prepare output data
for i, poem in enumerate(poems):
    output_data.append({
        'poem number': poem['poem number'],
        'poem': poem['poems'],
        'lda_topic': lda_topics[i],
        'keywords': topic_keywords[lda_topics[i]]  # Get top keywords for LDA topic
    })

# Save output to JSON
with open('/content/transformed_poems_combined_topics.json', 'w', encoding='utf-8') as file:
    json.dump(output_data, file, ensure_ascii=False, indent=4)

print("LDA and ParsBERT combined topic modeling complete! Check 'transformed_poems_combined_topics.json' for the result.")

unique_keywords_set = set()

for poem in output_data:
    keywords = poem['keywords']  # Get the keywords for each poem
    unique_keywords_set.update(keywords)  # Add keywords to the set for uniqueness

# Convert the set of unique keywords to a sorted list for better readability
unique_keywords_list = sorted(list(unique_keywords_set))

# Print unique keywords
print("Unique Keywords:")
print(len(unique_keywords_list))



"""## new way

"""

!pip install hazm

import json
from collections import defaultdict

# Define stop words
stop_words = set([
    "و", "از", "به", "در", "که", "این", "است", "با", "برای", "را", "نمی",
    "تا", "درست", "هم", "بود", "هر", "یک", "خود", "او", "تو", "من", "دل", "می", "ای", "ما", "کردم", "یا", "چه", "چرا", "حافظ", "شما",
    "آن", "علیه", "همچنان", "بر", "پس", "اگر", "هرگز",'دارد', 'دوست', 'رفت', 'سر', 'شد', 'می', 'نبود','آمده', 'ای', 'باد', 'جان', 'خواهد', 'خوش', 'دانست', 'زده', 'می', 'نیز', 'چون', 'کنید', 'گر','نرود', 'نمی', 'نیست', 'چو', 'کرد', 'کنند', 'گفت', 'گیرد'
])

# Load the original data
with open('/content/hafez_ghazal_poems_data.json', 'r', encoding='utf-8') as file:
    data = json.load(file)

# Organize poems by their poem number
grouped_poems = defaultdict(list)
for item in data:
    poem_number = item['poem number']
    poem_text = item['poem']
    grouped_poems[poem_number].append(poem_text)

# Function to remove stop words from a poem
def remove_stop_words(poem):
    words = poem.split()
    filtered_words = [word for word in words if word not in stop_words]
    return ' '.join(filtered_words)

# Process the dataset and remove stop words from each poem
transformed_data = [
    {
        'poem number': f'poem number {poem_number}',
        'poems': '\n '.join(remove_stop_words(poem) for poem in poems)  # Concatenate the poems into a single string
    }
    for poem_number, poems in grouped_poems.items()
]

# Save the modified data to a new JSON file
with open('/content/transformed_poems_no_stopwords.json', 'w', encoding='utf-8') as file:
    json.dump(transformed_data, file, ensure_ascii=False, indent=4)

print("Transformation complete! Check 'transformed_poems_no_stopwords.json' for the result.")

from bertopic import BERTopic
from sklearn.feature_extraction.text import CountVectorizer
from transformers import AutoTokenizer, AutoModel
import torch
import pandas as pd
import numpy as np
from hazm import Normalizer, WordTokenizer, stopwords_list

# Preprocess the Persian Poems
def preprocess_text(text):
    normalizer = Normalizer()
    tokenizer = WordTokenizer()
    # Normalize the text
    text = normalizer.normalize(text)
    # Tokenize the text and remove stop words
    stop_words = stopwords_list()
    tokens = tokenizer.tokenize(text)
    tokens = [word for word in tokens if word not in stop_words and len(word) > 1]
    return " ".join(tokens)

# Load the data
data = transformed_data[:10]

# Preprocess all poems
data_df = pd.DataFrame(data)
data_df['cleaned_poems'] = data_df['poems'].apply(preprocess_text)

# Step 3: Initialize the pre-trained BERT model for Persian (DistilBERT)
tokenizer = AutoTokenizer.from_pretrained("distilbert-base-multilingual-cased")
model = AutoModel.from_pretrained("distilbert-base-multilingual-cased")

def get_embeddings(texts):
    # Tokenize and encode the text to get embeddings
    inputs = tokenizer(texts, return_tensors='pt', truncation=True, padding=True, max_length=512)
    with torch.no_grad():
        embeddings = model(**inputs).last_hidden_state.mean(dim=1)
    # Convert to numpy array
    return embeddings.numpy()

# Get embeddings for all poems
embeddings = get_embeddings(data_df['cleaned_poems'].tolist())

# Convert embeddings to dense format
embeddings_dense = np.array(embeddings)

# Step 4: Apply BERTopic
topic_model = BERTopic(nr_topics=3, verbose=True)  # Use nr_topics instead of n_topics or n_components
topics, probs = topic_model.fit_transform(data_df['cleaned_poems'], embeddings_dense)

# Step 5: View the Topics
print("Topics:")
for i, topic in enumerate(set(topics)):
    print(f"Topic {i}: {topic_model.get_topic(topic)}")

"""##nmd

"""

!pip install bertopic
!pip install sentence-transformers

import json
from collections import defaultdict

# Define stop words
stop_words = set([
    "و", "از", "به", "در", "که", "این", "است", "با", "برای", "را", "نمی",
    "تا", "درست", "هم", "بود", "هر", "یک", "خود", "او", "تو", "من", "دل", "می", "ای", "ما", "کردم", "یا", "چه", "چرا", "حافظ", "شما",
    "آن", "علیه", "همچنان", "بر", "پس", "اگر", "هرگز",'دارد', 'دوست', 'رفت', 'سر', 'شد', 'می', 'نبود','آمده', 'ای', 'باد', 'جان', 'خواهد', 'خوش', 'دانست', 'زده', 'می', 'نیز', 'چون', 'کنید', 'گر','نرود', 'نمی', 'نیست', 'چو', 'کرد', 'کنند', 'گفت', 'گیرد'
])

# Load the original data
with open('/content/hafez_ghazal_poems_data.json', 'r', encoding='utf-8') as file:
    data = json.load(file)

# Organize poems by their poem number
grouped_poems = defaultdict(list)
for item in data:
    poem_number = item['poem number']
    poem_text = item['poem']
    grouped_poems[poem_number].append(poem_text)

# Function to remove stop words from a poem
def remove_stop_words(poem):
    words = poem.split()
    filtered_words = [word for word in words if word not in stop_words]
    return ' '.join(filtered_words)

# Process the dataset and remove stop words from each poem
transformed_data = [
    {
        'poem number': f'poem number {poem_number}',
        'poems': '\n '.join(remove_stop_words(poem) for poem in poems)  # Concatenate the poems into a single string
    }
    for poem_number, poems in grouped_poems.items()
]

# Save the modified data to a new JSON file
with open('/content/transformed_poems_no_stopwords.json', 'w', encoding='utf-8') as file:
    json.dump(transformed_data, file, ensure_ascii=False, indent=4)

print("Transformation complete! Check 'transformed_poems_no_stopwords.json' for the result.")

import pandas as pd
from bertopic import BERTopic
from sentence_transformers import SentenceTransformer
import json

# Example of your data (replace this with your actual data)
data = transformed_data[:10]

# Load the data into a DataFrame
df = pd.DataFrame(data)

# Initialize the sentence transformer model for embeddings
model = SentenceTransformer('paraphrase-multilingual-MiniLM-L12-v2')

# Generate embeddings for the poems
embeddings = model.encode(df['poems'].tolist(), show_progress_bar=True)

# Initialize BERTopic
topic_model = BERTopic(language="multilingual", nr_topics=3)  # You can adjust `nr_topics` as needed

# Fit the model to the poems and their embeddings
topics, probs = topic_model.fit_transform(df['poems'], embeddings)

# Create a list to hold the output in JSON format
output_data = []

# Iterate over each poem and its topic
for idx, row in df.iterrows():
    poem_data = {
        "poem number": row["poem number"],
        "poems": row["poems"],
        "topic": topics[idx],
        "keywords": [word[0] for word in topic_model.get_topic(topics[idx])]
    }
    output_data.append(poem_data)

# Convert the output data to JSON format
json_output = json.dumps(output_data, ensure_ascii=False, indent=4)

# Print the JSON output
print(json_output)





"""##new new"""

from bertopic import BERTopic
from collections import defaultdict

# Sample poem data (use your actual `new_json` variable here)
poems = transformed_data[:10]
# Initialize BERTopic model
topic_model = BERTopic(language="multilingual")

# Group poems by poet number and treat each poem as an individual document
grouped_poems = defaultdict(list)
for poem in poems:
    grouped_poems[poem["poem number"]].append(poem["poems"])

# Flatten poems for topic modeling and keep track of mappings
all_poems = [poem["poems"] for poem in poems]
topics, probs = topic_model.fit_transform(all_poems)

# Map each poem to its topics and add keywords to the explanation
for i, poem in enumerate(poems):
    topic_keywords = topic_model.get_topic(topics[i])
    # Get the top 3 keywords for the topic
    keywords_text = ', '.join([kw[0] for kw in topic_keywords[:3]]) if topic_keywords else "No keywords found"
    poem["explanation"] = keywords_text

# Display updated poems with keyword-based explanations
for poem in poems:
    print(f"Poem Number: {poem['poem number']}, Poem: {poem['poems']}, Explanation (Keywords): {poem['explanation']}")



"""#Stanza"""

import json
from collections import defaultdict

# Define stop words
stop_words = set([
    "و", "از", "به", "در", "که", "این", "است", "با", "برای", "را", "نمی",
    "تا", "درست", "هم", "بود", "هر", "یک", "خود", "او", "تو", "من", "دل", "می", "ای", "ما", "کردم", "یا", "چه", "چرا", "حافظ", "شما",
    "آن", "علیه", "همچنان", "بر", "پس", "اگر", "هرگز",'دارد', 'دوست', 'رفت', 'سر', 'شد', 'می', 'نبود','آمده', 'ای', 'باد', 'جان', 'خواهد', 'خوش', 'دانست', 'زده', 'می', 'نیز', 'چون', 'کنید', 'گر','نرود', 'نمی', 'نیست', 'چو', 'کرد', 'کنند', 'گفت', 'گیرد'
])

# Load the original data
with open('/content/hafez_ghazal_poems_data.json', 'r', encoding='utf-8') as file:
    data = json.load(file)

# Organize poems by their poem number
grouped_poems = defaultdict(list)
for item in data:
    poem_number = item['poem number']
    poem_text = item['poem']
    grouped_poems[poem_number].append(poem_text)

# Function to remove stop words from a poem
def remove_stop_words(poem):
    words = poem.split()
    filtered_words = [word for word in words if word not in stop_words]
    return ' '.join(filtered_words)

# Process the dataset and remove stop words from each poem
transformed_data = [
    {
        'poem number': f'poem number {poem_number}',
        'poems': '\n '.join(remove_stop_words(poem) for poem in poems)  # Concatenate the poems into a single string
    }
    for poem_number, poems in grouped_poems.items()
]

# Save the modified data to a new JSON file
with open('/content/transformed_poems_no_stopwords.json', 'w', encoding='utf-8') as file:
    json.dump(transformed_data, file, ensure_ascii=False, indent=4)

print("Transformation complete! Check 'transformed_poems_no_stopwords.json' for the result.")

!pip install stanza bertopic scikit-learn

import stanza
from bertopic import BERTopic
from sklearn.feature_extraction.text import CountVectorizer
import json

# Step 1: Load the Persian model in Stanza
stanza.download('fa')  # Download Persian model for Stanza
nlp = stanza.Pipeline('fa')  # Load the Persian pipeline

# Sample poems (replace this with your dataset)
poems = transformed_data[:10]

# Step 2: Preprocess the first 10 poems by keeping only relevant nouns, adjectives, and proper nouns
preprocessed_poems = []

for poem in poems:  # Only take the first 10 poems
    poem_text = poem["poems"]
    doc = nlp(poem_text)

    # Tokenize and lemmatize, keeping only nouns, adjectives, and proper nouns
    selected_words = []
    for sent in doc.sentences:
        for word in sent.words:
            # Keep nouns, adjectives, and proper nouns
            if word.pos in ['NOUN', 'ADJ', 'PROPN']:
                selected_words.append(word.lemma)

    # Join the selected words into a single string (suitable for topic modeling)
    preprocessed_poems.append(' '.join(selected_words))
print(preprocessed_poems)

# Step 3: Use BERTopic for topic modeling with a custom vectorizer
# Adjusting the vectorizer to include n-grams and remove more stop words
vectorizer = CountVectorizer(stop_words='english', ngram_range=(1, 2))  # Add bigrams (1,2) to capture phrases
topic_model = BERTopic(language="multilingual", vectorizer_model=vectorizer)
topics, probs = topic_model.fit_transform(preprocessed_poems)

# Step 4: Extract Top 3 Keywords for Each Poem
output_data = []

for i, poem in enumerate(poems):
    # topics, probs = topic_model.fit_transform(preprocessed_poems[i])

    topic = topics[i]
    # Extract top 3 keywords for the topic of the current poem
    top_keywords = [word for word, _ in topic_model.get_topic(topic)[:3]]

    # Structure the data for the poem with keywords and the original poem
    output_data.append({
        "poem number": poem["poem number"],
        "poem": poem["poems"],  # Include the original poem text
        "keywords": top_keywords
    })
    print(top_keywords)

# Step 5: Save the output as a JSON file
output_file = 'poem_topics_with_text.json'

with open(output_file, 'w', encoding='utf-8') as f:
    json.dump(output_data, f, ensure_ascii=False, indent=4)

print(f"JSON file saved as {output_file}")

import stanza
from bertopic import BERTopic
from sklearn.feature_extraction.text import CountVectorizer
import json

# Step 1: Load the Persian model in Stanza
stanza.download('fa')  # Download Persian model for Stanza
nlp = stanza.Pipeline('fa')  # Load the Persian pipeline

# Sample poems (replace this with your dataset)
poems = transformed_data[:10]

# Step 2: Preprocess the first 10 poems by keeping only relevant nouns, adjectives, and proper nouns
preprocessed_poems = []

for poem in poems:  # Only take the first 10 poems
    poem_text = poem["poems"]

    for beit in poem_text.split('\n'):
          # print(beit)
          doc = nlp(beit)


    # Tokenize and lemmatize, keeping only nouns, adjectives, and proper nouns
    selected_words = []
    for sent in doc.sentences:
        for word in sent.words:
            # Keep nouns, adjectives, and proper nouns
            if word.pos in ['NOUN', 'ADJ', 'PROPN']:
                selected_words.append(word.lemma)

    # Join the selected words into a single string (suitable for topic modeling)
    preprocessed_poems.append(' '.join(selected_words))
print(preprocessed_poems)

# Step 3: Use BERTopic for topic modeling with a custom vectorizer
# Adjusting the vectorizer to include n-grams and remove more stop words
vectorizer = CountVectorizer(stop_words='english', ngram_range=(1, 2))  # Add bigrams (1,2) to capture phrases
topic_model = BERTopic(language="multilingual", vectorizer_model=vectorizer)
topics, probs = topic_model.fit_transform(preprocessed_poems)

# Step 4: Extract Top 3 Keywords for Each Poem
output_data = []

for i, poem in enumerate(poems):
    # topics, probs = topic_model.fit_transform(preprocessed_poems[i])

    topic = topics[i]
    # Extract top 3 keywords for the topic of the current poem
    top_keywords = [word for word, _ in topic_model.get_topic(topic)[:3]]

    # Structure the data for the poem with keywords and the original poem
    output_data.append({
        "poem number": poem["poem number"],
        "poem": poem["poems"],  # Include the original poem text
        "keywords": top_keywords
    })
    print(top_keywords)

# Step 5: Save the output as a JSON file
output_file = 'poem_topics_with_text.json'

with open(output_file, 'w', encoding='utf-8') as f:
    json.dump(output_data, f, ensure_ascii=False, indent=4)

print(f"JSON file saved as {output_file}")

import stanza
from bertopic import BERTopic
from sklearn.feature_extraction.text import CountVectorizer
import json

# Step 1: Load the Persian model in Stanza
stanza.download('fa')  # Download Persian model for Stanza
nlp = stanza.Pipeline('fa')  # Load the Persian pipeline

# Sample poems (replace this with your dataset)
poems = transformed_data[:10]

# Step 2: Preprocess the first 10 poems by keeping only relevant nouns, adjectives, and proper nouns
preprocessed_poems = []
output_data = []


for i, poem in enumerate(poems):  # Only take the first 10 poems
    poem_text = poem["poems"]
    for beit in poem_text.split('\n'):
          # print(beit)
          doc = nlp(beit)
    # Tokenize and lemmatize, keeping only nouns, adjectives, and proper nouns
          selected_words = []
          for sent in doc.sentences:
                for word in sent.words:
                    # Keep nouns, adjectives, and proper nouns
                    if word.pos in ['NOUN', 'ADJ', 'PROPN']:
                        selected_words.append(word.lemma)
                        preprocessed_poems.append(' '.join(selected_words))


    # Join the selected words into a single string (suitable for topic modeling)
    # preprocessed_poems.append(' '.join(selected_words))
    print(preprocessed_poems)
    vectorizer = CountVectorizer(stop_words='english', ngram_range=(1, 2))  # Add bigrams (1,2) to capture phrases
    topic_model = BERTopic(language="multilingual", vectorizer_model=vectorizer)
    topics, probs = topic_model.fit_transform(preprocessed_poems)
     # Extract top 3 keywords for the topic of the current poem
    top_keywords = [word for word, _ in topic_model.get_topic(topics)[:3]]

    # Structure the data for the poem with keywords and the original poem
    output_data.append({
        "poem number": poem["poem number"],
        "poem": poem["poems"],  # Include the original poem text
        "keywords": top_keywords
    })
    print(top_keywords)


# Step 3: Use BERTopic for topic modeling with a custom vectorizer
# Adjusting the vectorizer to include n-grams and remove more stop words


# Step 4: Extract Top 3 Keywords for Each Poem

# for i, poem in enumerate(poems):
#     # topics, probs = topic_model.fit_transform(preprocessed_poems[i])

#     topic = topics[i]
#     # Extract top 3 keywords for the topic of the current poem
#     top_keywords = [word for word, _ in topic_model.get_topic(topic)[:3]]

#     # Structure the data for the poem with keywords and the original poem
#     output_data.append({
#         "poem number": poem["poem number"],
#         "poem": poem["poems"],  # Include the original poem text
#         "keywords": top_keywords
#     })
#     print(top_keywords)

# Step 5: Save the output as a JSON file
output_file = 'poem_topics_with_text.json'

with open(output_file, 'w', encoding='utf-8') as f:
    json.dump(output_data, f, ensure_ascii=False, indent=4)

print(f"JSON file saved as {output_file}")

import re

text=transformed_data[0]['poems'].replace("\u2000", "")
text = re.sub(r'[\u2000-\u200D]', '', text)  # Removes only Unicode characters in the range U+2000 to U+200D
text = " || ".join(text.split("\n"))
text=''.join(char for char in text if char.isprintable())
text

import json
import stanza
from bertopic import BERTopic
from sklearn.feature_extraction.text import CountVectorizer

# Initialize Stanza pipeline for Persian processing
stanza.download('fa')
nlp = stanza.Pipeline('fa')

# Load the dataset
file_path = '/content/transformed_poems_no_stopwords.json'
with open(file_path, 'r', encoding='utf-8') as f:
    data = json.load(f)

# Preprocess poems using Stanza
def preprocess_poems(poems):
    preprocessed = []
    for poem in poems:
        doc = nlp(poem)
        tokens = [word.text for sentence in doc.sentences for word in sentence.words]
        preprocessed.append(" ".join(tokens))
    return preprocessed

# Extract all poems from data
poems = [item['poems'] for item in data]
preprocessed_poems = preprocess_poems(poems)

# Set up CountVectorizer for use in BERTopic
vectorizer_model = CountVectorizer(ngram_range=(1, 2), stop_words=None)

# Use BERTopic to generate topics and extract top 3 keywords
topic_model = BERTopic(vectorizer_model=vectorizer_model)
topics, _ = topic_model.fit_transform(preprocessed_poems)

# Retrieve the top 3 keywords for each poem's topic
poem_keywords = []
for i, poem in enumerate(data):
    topic = topics[i]
    keywords = topic_model.get_topic(topic)[:3] if topic != -1 else [("N/A", 0)]
    keywords = [word for word, _ in keywords]
    poem_keywords.append({
        "poem number": poem['poem number'],
        "poems": poem['poems'],
        "keywords": keywords
    })

# Save the results to a JSON file
output_path = 'poem_keywords.json'  # Path to your output file
with open(output_path, 'w', encoding='utf-8') as f:
    json.dump(poem_keywords, f, ensure_ascii=False, indent=4)

print(f"Keywords extracted and saved to {output_path}")



"""#key word extraction using LLMs

##Gemma
"""

!pip install gradio_client

from gradio_client import Client

client = Client("keitokei1994/gemma-2-27b-it-Q4_K_M-chat-test")
result = client.predict(
		message='"poems": "اَلا اَیُّهَا السّاقی اَدِرْ کَأسَاً ناوِلْها | عشق آسان نمود اوّل ولی افتاد مشکل‌ها\n بویِ نافه‌ای کآخر صبا زان طُرّه بُگشاید | ز تابِ جَعدِ مُشکینش خون افتاد دل‌ها\n مرا منزلِ جانان اَمنِ عیش، دَم | جَرَس فریاد می‌دارد بَربندید مَحمِل‌ها\n مِی سجّاده رنگین کُن گَرَت پیرِ مُغان گوید | سالِک بی‌خبر نَبْوَد ز راه رسمِ منزل‌ها\n شبِ تاریک بیمِ موج گِردابی چنین هایل | کجا دانند حالِ سبکبارانِ ساحل‌ها؟\n همه کارم ز خودکامی بدنامی کشید آخر | نهان کِی مانَد رازی کَزو سازند مَحفِل‌ها؟\n حضوری همی‌خواهی غایب مشو | مَتٰی تَلْقَ مَنْ تَهْویٰ دَعِ الدُّنْیا اَهْمِلْها"',
		request="gemma-2-27b-it-Q4_K_M.gguf",
		param_3="You are a helpful assistant that your job is to give just 3 Persian keywords not the definition that represents the poem given.",
		param_4=2048,
		param_5=0.7,
		param_6=0.95,
		param_7=40,
		param_8=1.1,
		api_name="/chat"
)
print(result)

from gradio_client import Client

client = Client("keitokei1994/gemma-2-27b-it-Q4_K_M-chat-test")
result = client.predict(
		message='"poems": "اَلا اَیُّهَا السّاقی اَدِرْ کَأسَاً ناوِلْها | عشق آسان نمود اوّل ولی افتاد مشکل‌ها\n بویِ نافه‌ای کآخر صبا زان طُرّه بُگشاید | ز تابِ جَعدِ مُشکینش خون افتاد دل‌ها\n مرا منزلِ جانان اَمنِ عیش، دَم | جَرَس فریاد می‌دارد بَربندید مَحمِل‌ها\n مِی سجّاده رنگین کُن گَرَت پیرِ مُغان گوید | سالِک بی‌خبر نَبْوَد ز راه رسمِ منزل‌ها\n شبِ تاریک بیمِ موج گِردابی چنین هایل | کجا دانند حالِ سبکبارانِ ساحل‌ها؟\n همه کارم ز خودکامی بدنامی کشید آخر | نهان کِی مانَد رازی کَزو سازند مَحفِل‌ها؟\n حضوری همی‌خواهی غایب مشو | مَتٰی تَلْقَ مَنْ تَهْویٰ دَعِ الدُّنْیا اَهْمِلْها"',
		request="gemma-2-27b-it-Q4_K_M.gguf",
		param_3="You are a helpful assistant that your job is to give just 3 Persian keywords not the definition in the list format that represents the poem given.",
		param_4=2048,
		param_5=0.7,
		param_6=0.95,
		param_7=40,
		param_8=1.1,
		api_name="/chat"
)
print(result)

from gradio_client import Client

client = Client("keitokei1994/gemma-2-27b-it-Q4_K_M-chat-test")
result = client.predict(
		message='"poems": "صلاحِ کار کجا منِ خراب کجا؟ | ببین تفاوتِ رَه کز کجاست کجا\n دلم ز صومعه بگرفت خِرقِهٔ سالوس | کجاست دیرِ مُغان شرابِ ناب کجا؟\n نسبت به‌ رندی صَلاح تقوا را؟ | سماعِ وَعظ کجا نغمهٔ رَباب کجا؟\n ز رویِ دلِ دشمنان دریابد؟ | چراغِ مُرده کجا شمعِ آفتاب کجا؟\n کُحلِ بینشِ خاکِ آستانِ شماست | کجا رویم بفرما ازین جناب کجا؟\n مَبین سیبِ زَنَخدان چاه راه | کجا همی‌ رَوی بدین شتاب کجا؟\n بِشُد! یادِ خوشش روزگارِ وصال | کِرِشمه کجا عِتاب کجا؟\n قرار خواب ز طمع مدار | قرار چیست صبوری کدام خواب کجا؟"',
		request="gemma-2-27b-it-Q4_K_M.gguf",
		param_3="You are a helpful assistant that your job is to give just 3 Persian keywords not the definition in the list format that represents the poem given.",
		param_4=2048,
		param_5=0.7,
		param_6=0.95,
		param_7=40,
		param_8=1.1,
		api_name="/chat"
)
print(result)

from gradio_client import Client

client = Client("keitokei1994/gemma-2-27b-it-Q4_K_M-chat-test")
result = client.predict(
		message='"poems": "تُرکِ شیرازی به‌‌ دست‌ آرَد دلِ | خال هِندویَش بَخشَم سمرقند بُخارا\n بده ساقی مِیِ باقی جَنَّت نخواهی یافت | کنارِ آب رُکن‌آباد گُل‌گَشتِ مُصَلّا\n فَغان کاین لولیانِ شوخِ شیرین‌کارِ شهرآشوب | چنان بردند صبر تُرکان خوانِ یَغما\n ز عشقِ ناتمامِ جمالِ یار مُستَغنی‌ | آب رنگ خال خط حاجت رویِ زیبا\n مَن حُسنِ روزاَفزون یوسُف داشت دانستم | عشق پردهٔ عِصمت بُرون آرَد زُلِیخا\n دشنام فرمایی گَر نفرین دعا گویم | جوابِ تلخ می‌زیبد لبِ لَعلِ شِکرخا\n نصیحت گوش کن جانا دوست‌تر دارند | جوانانِ سعادتمند پندِ پیرِ دانا\n حَدیث مُطرب مِی گو رازِ دَهر کمتر جو | کس نَگشود نَگشاید حکمت مُعمّا\n غزل گفتی دُر سُفتی بیا بخوان | نظمِ اَفشانَد فَلَک عِقد ثُریّا"',
		request="gemma-2-27b-it-Q4_K_M.gguf",
		param_3="You are a helpful assistant that your job is to give just 3 Persian keywords not the definition with '**' before them that represents the poem given.",
		param_4=2048,
		param_5=0.7,
		param_6=0.95,
		param_7=40,
		param_8=1.1,
		api_name="/chat"
)
print(result)

res=result.split("\n")

res[4].split("**")[1]

"""##Qwen 2.5

###Learning Qwen
"""

!pip install gradio_client

# from gradio_client import Client

# client = Client("llamameta/Qwen2.5-Coder-32B-Instruct-Chat-Assistant")
# result = client.predict(
# 		message='"poems": "اَلا اَیُّهَا السّاقی اَدِرْ کَأسَاً ناوِلْها | عشق آسان نمود اوّل ولی افتاد مشکل‌ها\n بویِ نافه‌ای کآخر صبا زان طُرّه بُگشاید | ز تابِ جَعدِ مُشکینش خون افتاد دل‌ها\n مرا منزلِ جانان اَمنِ عیش، دَم | جَرَس فریاد می‌دارد بَربندید مَحمِل‌ها\n مِی سجّاده رنگین کُن گَرَت پیرِ مُغان گوید | سالِک بی‌خبر نَبْوَد ز راه رسمِ منزل‌ها\n شبِ تاریک بیمِ موج گِردابی چنین هایل | کجا دانند حالِ سبکبارانِ ساحل‌ها؟\n همه کارم ز خودکامی بدنامی کشید آخر | نهان کِی مانَد رازی کَزو سازند مَحفِل‌ها؟\n حضوری همی‌خواهی غایب مشو | مَتٰی تَلْقَ مَنْ تَهْویٰ دَعِ الدُّنْیا اَهْمِلْها"',
# 		system_message="You are Qwen, created by Alibaba Cloud. You are a helpful assistant that will give 3 Persian keywords without explnation that represents the poem from massage in python list format.",
# 		max_tokens=18000,
# 		temperature=0.7,
# 		top_p=0.8,
# 		api_name="/chat"
# )
# print(result)

# from gradio_client import Client

# client = Client("llamameta/Qwen2.5-Coder-32B-Instruct-Chat-Assistant")
# result = client.predict(
# 		message='"poems": "صلاحِ کار کجا منِ خراب کجا؟ | ببین تفاوتِ رَه کز کجاست کجا\n دلم ز صومعه بگرفت خِرقِهٔ سالوس | کجاست دیرِ مُغان شرابِ ناب کجا؟\n نسبت به‌ رندی صَلاح تقوا را؟ | سماعِ وَعظ کجا نغمهٔ رَباب کجا؟\n ز رویِ دلِ دشمنان دریابد؟ | چراغِ مُرده کجا شمعِ آفتاب کجا؟\n کُحلِ بینشِ خاکِ آستانِ شماست | کجا رویم بفرما ازین جناب کجا؟\n مَبین سیبِ زَنَخدان چاه راه | کجا همی‌ رَوی بدین شتاب کجا؟\n بِشُد! یادِ خوشش روزگارِ وصال | کِرِشمه کجا عِتاب کجا؟\n قرار خواب ز طمع مدار | قرار چیست صبوری کدام خواب کجا؟"',
# 		system_message="You are Qwen, created by Alibaba Cloud. You are a helpful assistant that will give 3 Persian keywords without explnation that represents the poem from massage in python list format.",
# 		max_tokens=18000,
# 		temperature=0.7,
# 		top_p=0.8,
# 		api_name="/chat"
# )
# print(result)

# from gradio_client import Client

# client = Client("llamameta/Qwen2.5-Coder-32B-Instruct-Chat-Assistant")
# result = client.predict(
# 		message='"poems": "تُرکِ شیرازی به‌‌ دست‌ آرَد دلِ | خال هِندویَش بَخشَم سمرقند بُخارا\n بده ساقی مِیِ باقی جَنَّت نخواهی یافت | کنارِ آب رُکن‌آباد گُل‌گَشتِ مُصَلّا\n فَغان کاین لولیانِ شوخِ شیرین‌کارِ شهرآشوب | چنان بردند صبر تُرکان خوانِ یَغما\n ز عشقِ ناتمامِ جمالِ یار مُستَغنی‌ | آب رنگ خال خط حاجت رویِ زیبا\n مَن حُسنِ روزاَفزون یوسُف داشت دانستم | عشق پردهٔ عِصمت بُرون آرَد زُلِیخا\n دشنام فرمایی گَر نفرین دعا گویم | جوابِ تلخ می‌زیبد لبِ لَعلِ شِکرخا\n نصیحت گوش کن جانا دوست‌تر دارند | جوانانِ سعادتمند پندِ پیرِ دانا\n حَدیث مُطرب مِی گو رازِ دَهر کمتر جو | کس نَگشود نَگشاید حکمت مُعمّا\n غزل گفتی دُر سُفتی بیا بخوان | نظمِ اَفشانَد فَلَک عِقد ثُریّا"',
# 		system_message="You are Qwen, created by Alibaba Cloud. You are a helpful assistant that will give 3 Persian keywords without explnation that represents the poem from massage in python list format.",
# 		max_tokens=18000,
# 		temperature=0.7,
# 		top_p=0.8,
# 		api_name="/chat"
# )
# print(result)

# from gradio_client import Client

# client = Client("llamameta/Qwen2.5-Coder-32B-Instruct-Chat-Assistant")
# result = client.predict(
# 		message='"poems": "صبا لُطف بگو غزالِ رَعنا | سَر کوه بیابان، داده‌ای\n شِکرفُروش عُمرَش دراز | تَفَقُّدی نَکُنَد طوطیِ شِکرخا را؟\n غرورِ حُسنت اجازَت مَگَر نداد اِی گُل | پُرسِشی نَکُنی عَندَلیبِ شِیدا را؟\n خُلق لُطف تَوان صیدِ اهلِ نَظَر | بند دام نَگیرَند مرغِ دانا\n نَدانَمَ ازْ سبب رنگِ آشنایی | سَهی‌قَدانِ سیَه‌‌چشمِ ماه‌سیما\n حبیب نِشینی باده پِیمایی | یاد دار مُحِبّانِ بادپیما\n جُز قَدَر نَتوان جَمالِ عیب | وضع مِهر وفا رویِ زیبا\n آسمان نه عجب، گَر گفتهٔ | سُرودِ زُهره رقص آوَرَد مَسیحا"',
# 		system_message="You are Qwen, created by Alibaba Cloud. You are a helpful assistant that will give 3 Persian keywords without explnation that represents the poem from massage in python list format.",
# 		max_tokens=18000,
# 		temperature=0.7,
# 		top_p=0.8,
# 		api_name="/chat"
# )
# print(result)

# from typing import List, Dict
# from gradio_client import Client



# client = Client("llamameta/Qwen2.5-Coder-32B-Instruct-Chat-Assistant")
# # Mock function to simulate API call
# def get_keywords(poem: str) -> List[str]:
#     response = client.predict(
#         message=f'"poems": "{poem}"',
#         system_message="You are Qwen, created by Alibaba Cloud. You are a helpful assistant that will give 3 Persian keywords without explanation that represents the poem from the message in python list format.",
#         max_tokens=50,
#         temperature=0.7,
#         top_p=0.8,
#         api_name="/chat"
#     )
#     print(response)
#     # res= [response.split('"')[1],response.split('"')[3],response.split('"')[5]]
#     # print(response.split('"'))
#     print(response.split("'")[1],response.split("'")[3],response.split("'")[5])
#     return (response.split("'")[1],response.split("'")[3],response.split("'")[5])

# # Your initial data
# with open('/content/transformed_poems_no_stopwords.json', 'r', encoding='utf-8') as file:
#     data = json.load(file)
# # Loop through the data to process each poem
# counter=1
# for entry in data:
#     poem_text = entry.get("poems", "")
#     if poem_text:
#         # Get 3 keywords from the API
#         keywords = get_keywords(poem_text)
#         # Add keywords to the entry
#         entry["keywords"] = keywords
#         print(counter)
#         counter += 1

# output_file = "poems_with_keywords.json"
# with open(output_file, "w", encoding="utf-8") as f:
#     json.dump(data, f, ensure_ascii=False, indent=4)

# print(f"Data saved to {output_file}")

# import json
# from typing import List
# from gradio_client import Client

# client = Client("llamameta/Qwen2.5-Coder-32B-Instruct-Chat-Assistant")

# # Function to truncate text to fit within the token limit
# def truncate_text(text: str, max_tokens: int = 32256) -> str:
#     # A simplistic truncation that ensures the text is within character limits.
#     # Use a tokenizer for more precise token-level truncation.
#     return text[:max_tokens]

# # Mock function to simulate API call
# def get_keywords(poem: str) -> List[str]:
#     truncated_poem = truncate_text(poem, max_tokens=32000)  # Slightly below the limit for safety
#     response = client.predict(
#         message=f'"poems": "{truncated_poem}"',
#         system_message="You are Qwen, created by Alibaba Cloud. You are a helpful assistant that will give 3 Persian keywords without explanation that represents the poem from the message in python list format.",
#         max_tokens=50,
#         temperature=0.7,
#         top_p=0.8,
#         api_name="/chat"
#     )
#     print(response)
#     keywords = [response.split("'")[1], response.split("'")[3], response.split("'")[5]]
#     print(keywords)
#     return keywords

# # Your initial data
# with open('/content/transformed_poems_no_stopwords.json', 'r', encoding='utf-8') as file:
#     data = json.load(file)

# # Loop through the data to process each poem
# for entry in data:
#     poem_text = entry.get("poems", "")
#     if poem_text:
#         # Get 3 keywords from the API
#         keywords = get_keywords(poem_text)
#         # Add keywords to the entry
#         entry["keywords"] = keywords

# output_file = "poems_with_keywords.json"
# with open(output_file, "w", encoding="utf-8") as f:
#     json.dump(data, f, ensure_ascii=False, indent=4)

# print(f"Data saved to {output_file}")

"""###First Part"""

import json
from typing import List
from gradio_client import Client

client = Client("llamameta/Qwen2.5-Coder-32B-Instruct-Chat-Assistant")

# Function to truncate text to fit within the token limit
def truncate_text(text: str, max_tokens: int = 32000) -> str:
    # Truncate based on character length as a rough approximation
    return text[:max_tokens]

# Function to get keywords
def get_keywords(poem: str) -> List[str]:
    truncated_poem = truncate_text(poem, max_tokens=32000)  # Leave space for system tokens
    try:
        response = client.predict(
            message=f'"poems": "{truncated_poem}"',
            system_message="You are Qwen, created by Alibaba Cloud. You are a helpful assistant that will give 5 Persian keywords without explanation that represents the poem from the message in python list format.",
            max_tokens=50,
            temperature=0.7,
            top_p=0.8,
            api_name="/chat"
        )
        # Extract keywords from response
        keywords = response.strip("[]").replace("'", "").split(", ")
        print(keywords)
        return keywords[:5]  # Ensure only 3 keywords
    except Exception as e:
        print(f"Error processing poem: {e}")
        return []

# Read the data
with open('/content/transformed_poems_no_stopwords.json', 'r', encoding='utf-8') as file:
    data = json.load(file)

# Process poems
counter = 1
for entry in data[:90]:
    poem_text = entry.get("poems", "")
    if poem_text:
        # Get 3 keywords
        keywords = get_keywords(poem_text)
        entry["keywords"] = keywords
        print(counter)
        counter += 1
# for entry in data[90:180]:
#     poem_text = entry.get("poems", "")
#     if poem_text:
#         # Get 3 keywords
#         keywords = get_keywords(poem_text)
#         entry["keywords"] = keywords
#         print(counter)
#         counter += 1
# for entry in data[180:270]:
#     poem_text = entry.get("poems", "")
#     if poem_text:
#         # Get 3 keywords
#         keywords = get_keywords(poem_text)
#         entry["keywords"] = keywords
#         print(counter)
#         counter += 1
# for entry in data[270:360]:
#     poem_text = entry.get("poems", "")
#     if poem_text:
#         # Get 3 keywords
#         keywords = get_keywords(poem_text)
#         entry["keywords"] = keywords
#         print(counter)
#         counter += 1
# for entry in data[360:450]:
#     poem_text = entry.get("poems", "")
#     if poem_text:
#         # Get 3 keywords
#         keywords = get_keywords(poem_text)
#         entry["keywords"] = keywords
#         print(counter)
#         counter += 1
# for entry in data[450:]:
#     poem_text = entry.get("poems", "")
#     if poem_text:
#         # Get 3 keywords
#         keywords = get_keywords(poem_text)
#         entry["keywords"] = keywords
#         print(counter)
#         counter += 1


# Save the results
# output_file = "poems_with_keywords.json"
# with open(output_file, "w", encoding="utf-8") as f:
#     json.dump(data, f, ensure_ascii=False, indent=4)

# print(f"Data saved to {output_file}")

"""###Second Part"""

client = Client("llamameta/Qwen2.5-Coder-32B-Instruct-Chat-Assistant")

# Function to truncate text to fit within the token limit
def truncate_text(text: str, max_tokens: int = 32000) -> str:
    # Truncate based on character length as a rough approximation
    return text[:max_tokens]

# Function to get keywords
def get_keywords(poem: str) -> List[str]:
    truncated_poem = truncate_text(poem, max_tokens=32000)  # Leave space for system tokens
    try:
        response = client.predict(
            message=f'"poems": "{truncated_poem}"',
            system_message="You are Qwen, created by Alibaba Cloud. You are a helpful assistant that will give 5 Persian keywords without explanation that represents the poem from the message in python list format.",
            max_tokens=50,
            temperature=0.7,
            top_p=0.8,
            api_name="/chat"
        )
        # Extract keywords from response
        keywords = response.strip("[]").replace("'", "").split(", ")
        print(keywords)
        return keywords[:5]  # Ensure only 3 keywords
    except Exception as e:
        print(f"Error processing poem: {e}")
        return []

for entry in data[90:180]:
    poem_text = entry.get("poems", "")
    if poem_text:
        # Get 3 keywords
        keywords = get_keywords(poem_text)
        entry["keywords"] = keywords
        print(counter)
        counter += 1

"""###3rd Part"""

from typing import List
import traceback  # For better error logging

# Initialize the client
client = Client("llamameta/Qwen2.5-Coder-32B-Instruct-Chat-Assistant")

# Function to truncate text based on token approximation
def truncate_text(text: str, max_tokens: int = 32000) -> str:
    # Truncate based on character length as a rough approximation
    return text[:max_tokens]

# Function to get keywords from a poem
def get_keywords(poem: str) -> List[str]:
    truncated_poem = truncate_text(poem, max_tokens=32000)  # Adjust token space if needed
    try:
        response = client.predict(
            message=f'"poems": "{truncated_poem}"',
            system_message=(
                "You are Qwen, created by Alibaba Cloud. You are a helpful assistant that will give 5 Persian "
                "keywords without explanation that represent the poem in Python list format."
            ),
            max_tokens=50,
            temperature=0.7,
            top_p=0.8,
            api_name="/chat"
        )
        # Extract keywords from response
        keywords = response.strip("[]").replace("'", "").split(", ")
        return keywords[:5]  # Ensure only 5 keywords
    except Exception as e:
        print(f"Error processing poem: {e}")
        print(traceback.format_exc())  # Log full traceback
        return []

# Process dataset
# counter = 0  # Initialize counter
for entry in data[180:270]:
    poem_text = entry.get("poems", "")
    if poem_text:
        # Get 5 keywords
        keywords = get_keywords(poem_text)
        entry["keywords"] = keywords
        print(f"Processed entry {counter}: {keywords}")
        counter += 1

"""###4th Part"""

client = Client("llamameta/Qwen2.5-Coder-32B-Instruct-Chat-Assistant")

# Function to truncate text to fit within the token limit
def truncate_text(text: str, max_tokens: int = 32000) -> str:
    # Truncate based on character length as a rough approximation
    return text[:max_tokens]

# Function to get keywords
def get_keywords(poem: str) -> List[str]:
    truncated_poem = truncate_text(poem, max_tokens=32000)  # Leave space for system tokens
    try:
        response = client.predict(
            message=f'"poems": "{truncated_poem}"',
            system_message="You are Qwen, created by Alibaba Cloud. You are a helpful assistant that will give 5 Persian keywords without explanation that represents the poem from the message in python list format.",
            max_tokens=50,
            temperature=0.7,
            top_p=0.8,
            api_name="/chat"
        )
        # Extract keywords from response
        keywords = response.strip("[]").replace("'", "").split(", ")
        print(keywords)
        return keywords[:5]  # Ensure only 3 keywords
    except Exception as e:
        print(f"Error processing poem: {e}")
        return []
counter = 271
for entry in data[270:350]:
    poem_text = entry.get("poems", "")
    if poem_text:
        # Get 3 keywords
        keywords = get_keywords(poem_text)
        entry["keywords"] = keywords
        print(counter)
        counter += 1

"""###5th Part"""

client = Client("llamameta/Qwen2.5-Coder-32B-Instruct-Chat-Assistant")

# Function to truncate text to fit within the token limit
def truncate_text(text: str, max_tokens: int = 32000) -> str:
    # Truncate based on character length as a rough approximation
    return text[:max_tokens]

# Function to get keywords
def get_keywords(poem: str) -> List[str]:
    truncated_poem = truncate_text(poem, max_tokens=32000)  # Leave space for system tokens
    try:
        response = client.predict(
            message=f'"poems": "{truncated_poem}"',
            system_message="You are Qwen, created by Alibaba Cloud. You are a helpful assistant that will give 5 Persian keywords without explanation that represents the poem from the message in python list format.",
            max_tokens=50,
            temperature=0.7,
            top_p=0.8,
            api_name="/chat"
        )
        # Extract keywords from response
        keywords = response.strip("[]").replace("'", "").split(", ")
        print(keywords)
        return keywords[:5]  # Ensure only 3 keywords
    except Exception as e:
        print(f"Error processing poem: {e}")
        return []

for entry in data[350:410]:
    poem_text = entry.get("poems", "")
    if poem_text:
        # Get 3 keywords
        keywords = get_keywords(poem_text)
        entry["keywords"] = keywords
        print(counter)
        counter += 1

"""###Last part"""

client = Client("llamameta/Qwen2.5-Coder-32B-Instruct-Chat-Assistant")

# Function to truncate text to fit within the token limit
def truncate_text(text: str, max_tokens: int = 32000) -> str:
    # Truncate based on character length as a rough approximation
    return text[:max_tokens]

# Function to get keywords
def get_keywords(poem: str) -> List[str]:
    truncated_poem = truncate_text(poem, max_tokens=32000)  # Leave space for system tokens
    try:
        response = client.predict(
            message=f'"poems": "{truncated_poem}"',
            system_message="You are Qwen, created by Alibaba Cloud. You are a helpful assistant that will give 5 Persian keywords without explanation that represents the poem from the message in python list format.",
            max_tokens=50,
            temperature=0.7,
            top_p=0.8,
            api_name="/chat"
        )
        # Extract keywords from response
        keywords = response.strip("[]").replace("'", "").split(", ")
        print(keywords)
        return keywords[:5]  # Ensure only 3 keywords
    except Exception as e:
        print(f"Error processing poem: {e}")
        return []

for entry in data[410:]:
    poem_text = entry.get("poems", "")
    if poem_text:
        # Get 3 keywords
        keywords = get_keywords(poem_text)
        entry["keywords"] = keywords
        print(counter)
        counter += 1


output_file = "poems_with_keywords.json"
with open(output_file, "w", encoding="utf-8") as f:
    json.dump(data, f, ensure_ascii=False, indent=4)

print(f"Data saved to {output_file}")

# import json
# import re
# from typing import List
# from gradio_client import Client

# client = Client("llamameta/Qwen2.5-Coder-32B-Instruct-Chat-Assistant")

# # Function to clean and truncate the input text
# def clean_and_truncate(text: str, max_tokens: int = 32000, buffer: int = 500) -> str:
#     """
#     Cleans and truncates input text to fit within the token limit.
#     Args:
#         text: Input string to be processed.
#         max_tokens: Maximum tokens allowed by the API.
#         buffer: Reserved tokens for system and prompt overhead.
#     Returns:
#         Truncated string.
#     """
#     # Clean text by removing unnecessary whitespaces and invisible characters
#     text = re.sub(r'\s+', ' ', text).strip()
#     # Approximate token count (Persian: ~1 token = 1.5 characters)
#     char_limit = (max_tokens - buffer) * 1.5
#     return text[:int(char_limit)]

# # Function to extract keywords
# def get_keywords(poem: str) -> List[str]:
#     cleaned_poem = clean_and_truncate(poem)
#     try:
#         response = client.predict(
#             message=f'"poems": "{cleaned_poem}"',
#             system_message="You are Qwen, created by Alibaba Cloud. You are a helpful assistant that will give 3 Persian keywords without explanation that represents the poem from the message in python list format.",
#             max_tokens=50,
#             temperature=0.7,
#             top_p=0.8,
#             api_name="/chat"
#         )
#         # Extract and clean keywords
#         keywords = response.strip("[]").replace("'", "").split(", ")
#         print(keywords)
#         return keywords[:3]  # Return only the first 3 keywords
#     except Exception as e:
#         print(f"Error processing poem: {e}")
#         return []

# # Read the dataset
# with open('/content/transformed_poems_no_stopwords.json', 'r', encoding='utf-8') as file:
#     data = json.load(file)

# # Process each poem
# for entry in data:
#     poem_text = entry.get("poems", "")
#     if poem_text:
#         keywords = get_keywords(poem_text)
#         entry["keywords"] = keywords

# # Save the results
# output_file = "poems_with_keywords.json"
# with open(output_file, "w", encoding="utf-8") as f:
#     json.dump(data, f, ensure_ascii=False, indent=4)

# print(f"Processed data saved to {output_file}")

"""#Keyword extraction for each verse

"""

# Read the data
with open('/content/hafez_ghazal_poems_data.json', 'r', encoding='utf-8') as file:
    data = json.load(file)

# data[1]["poem"]
len(data)

import json
from typing import List
from gradio_client import Client

# Initialize the client
client = Client("llamameta/Qwen2.5-Coder-32B-Instruct-Chat-Assistant")

# Function to truncate text based on token approximation
def truncate_text(text: str, max_tokens: int = 32000) -> str:
    # Truncate based on character length as a rough approximation
    return text[:max_tokens]

# Function to get keywords from a poem
def get_keywords(poem: str) -> List[str]:
    truncated_poem = truncate_text(poem, max_tokens=32000)  # Adjust token space if needed
    try:
        response = client.predict(
            message=f'"poems": "{truncated_poem}"',
            system_message=(
                "You are Qwen, created by Alibaba Cloud. You are a helpful assistant that will give 3 Persian noun "
                "keywords without explanation that represent the poem in Python list format."
            ),
            max_tokens=50,
            temperature=0.7,
            top_p=0.8,
            api_name="/chat"
        )
        # Extract keywords from response
        keywords = response.strip("[]").replace("'", "").split(", ")
        return keywords[:3]  # Ensure only 5 keywords
    except Exception as e:
        print(f"Error processing poem: {e}")
        return []

# Process the data
counter = 1  # Initialize counter

for entry in data[:350]:
    poem_text = entry.get("poem", "")  # Extract the "poem" field
    if poem_text:
        # Get keywords for the poem
        keywords = get_keywords(poem_text)
        entry["keywords"] = keywords  # Add the keywords to the entry
        print(f"Processed poem {counter}: {keywords}")
        counter += 1

# Print updated data
print(json.dumps(data, ensure_ascii=False, indent=4))

import json
from typing import List
from gradio_client import Client

# Initialize the client
client = Client("llamameta/Qwen2.5-Coder-32B-Instruct-Chat-Assistant")

# Function to truncate text based on token approximation
def truncate_text(text: str, max_tokens: int = 32000) -> str:
    # Truncate based on character length as a rough approximation
    return text[:max_tokens]

# Function to get keywords from a poem
def get_keywords(poem: str) -> List[str]:
    truncated_poem = truncate_text(poem, max_tokens=32000)  # Adjust token space if needed
    try:
        response = client.predict(
            message=f'"poems": "{truncated_poem}"',
            system_message=(
                "You are Qwen, created by Alibaba Cloud. You are a helpful assistant that will give 3 Persian noun "
                "keywords without explanation that represent the poem in Python list format."
            ),
            max_tokens=50,
            temperature=0.7,
            top_p=0.8,
            api_name="/chat"
        )
        # Extract keywords from response
        keywords = response.strip("[]").replace("'", "").split(", ")
        return keywords[:3]  # Ensure only 5 keywords
    except Exception as e:
        print(f"Error processing poem: {e}")
        return []

# Process the data
# counter = 1  # Initialize counter

for entry in data[350:700]:
    poem_text = entry.get("poem", "")  # Extract the "poem" field
    if poem_text:
        # Get keywords for the poem
        keywords = get_keywords(poem_text)
        entry["keywords"] = keywords  # Add the keywords to the entry
        print(f"Processed poem {counter}: {keywords}")
        counter += 1

# Print updated data
# print(json.dumps(data, ensure_ascii=False, indent=4))

import json
from typing import List
from gradio_client import Client

# Initialize the client
client = Client("llamameta/Qwen2.5-Coder-32B-Instruct-Chat-Assistant")

# Function to truncate text based on token approximation
def truncate_text(text: str, max_tokens: int = 32000) -> str:
    # Truncate based on character length as a rough approximation
    return text[:max_tokens]

# Function to get keywords from a poem
def get_keywords(poem: str) -> List[str]:
    truncated_poem = truncate_text(poem, max_tokens=32000)  # Adjust token space if needed
    try:
        response = client.predict(
            message=f'"poems": "{truncated_poem}"',
            system_message=(
                "You are Qwen, created by Alibaba Cloud. You are a helpful assistant that will give 3 Persian noun"
                "keywords without explanation that represent the poem in Python list format."
            ),
            max_tokens=50,
            temperature=0.7,
            top_p=0.8,
            api_name="/chat"
        )
        # Extract keywords from response
        keywords = response.strip("[]").replace("'", "").split(", ")
        return keywords[:3]  # Ensure only 5 keywords
    except Exception as e:
        print(f"Error processing poem: {e}")
        return []

# Process the data
# counter = 1  # Initialize counter

for entry in data[700:1050]:
    poem_text = entry.get("poem", "")  # Extract the "poem" field
    if poem_text:
        # Get keywords for the poem
        keywords = get_keywords(poem_text)
        entry["keywords"] = keywords  # Add the keywords to the entry
        print(f"Processed poem {counter}: {keywords}")
        counter += 1

# Print updated data
# print(json.dumps(data, ensure_ascii=False, indent=4))

import json
from typing import List
from gradio_client import Client

# Initialize the client
client = Client("llamameta/Qwen2.5-Coder-32B-Instruct-Chat-Assistant")

# Function to truncate text based on token approximation
def truncate_text(text: str, max_tokens: int = 32000) -> str:
    # Truncate based on character length as a rough approximation
    return text[:max_tokens]

# Function to get keywords from a poem
def get_keywords(poem: str) -> List[str]:
    truncated_poem = truncate_text(poem, max_tokens=32000)  # Adjust token space if needed
    try:
        response = client.predict(
            message=f'"poems": "{truncated_poem}"',
            system_message=(
                "You are Qwen, created by Alibaba Cloud. You are a helpful assistant that will give 3 Persian noun "
                "keywords without explanation that represent the poem in Python list format."
            ),
            max_tokens=50,
            temperature=0.7,
            top_p=0.8,
            api_name="/chat"
        )
        # Extract keywords from response
        keywords = response.strip("[]").replace("'", "").split(", ")
        return keywords[:3]  # Ensure only 5 keywords
    except Exception as e:
        print(f"Error processing poem: {e}")
        return []

# Process the data
# counter = 1  # Initialize counter

for entry in data[1050:1400]:
    poem_text = entry.get("poem", "")  # Extract the "poem" field
    if poem_text:
        # Get keywords for the poem
        keywords = get_keywords(poem_text)
        entry["keywords"] = keywords  # Add the keywords to the entry
        print(f"Processed poem {counter}: {keywords}")
        counter += 1

# Print updated data
# print(json.dumps(data, ensure_ascii=False, indent=4))

import json
from typing import List
from gradio_client import Client

# Initialize the client
client = Client("llamameta/Qwen2.5-Coder-32B-Instruct-Chat-Assistant")

# Function to truncate text based on token approximation
def truncate_text(text: str, max_tokens: int = 32000) -> str:
    # Truncate based on character length as a rough approximation
    return text[:max_tokens]

# Function to get keywords from a poem
def get_keywords(poem: str) -> List[str]:
    truncated_poem = truncate_text(poem, max_tokens=32000)  # Adjust token space if needed
    try:
        response = client.predict(
            message=f'"poems": "{truncated_poem}"',
            system_message=(
                "You are Qwen, created by Alibaba Cloud. You are a helpful assistant that will give 3 Persian noun "
                "keywords without explanation that represent the poem in Python list format."
            ),
            max_tokens=50,
            temperature=0.7,
            top_p=0.8,
            api_name="/chat"
        )
        # Extract keywords from response
        keywords = response.strip("[]").replace("'", "").split(", ")
        return keywords[:3]  # Ensure only 5 keywords
    except Exception as e:
        print(f"Error processing poem: {e}")
        return []

# Process the data
# counter = 1  # Initialize counter

for entry in data[1399:1750]:
    poem_text = entry.get("poem", "")  # Extract the "poem" field
    if poem_text:
        # Get keywords for the poem
        keywords = get_keywords(poem_text)
        entry["keywords"] = keywords  # Add the keywords to the entry
        print(f"Processed poem {counter}: {keywords}")
        counter += 1

# Print updated data
# print(json.dumps(data, ensure_ascii=False, indent=4))

import json
from typing import List
from gradio_client import Client

# Initialize the client
client = Client("llamameta/Qwen2.5-Coder-32B-Instruct-Chat-Assistant")

# Function to truncate text based on token approximation
def truncate_text(text: str, max_tokens: int = 32000) -> str:
    # Truncate based on character length as a rough approximation
    return text[:max_tokens]

# Function to get keywords from a poem
def get_keywords(poem: str) -> List[str]:
    truncated_poem = truncate_text(poem, max_tokens=32000)  # Adjust token space if needed
    try:
        response = client.predict(
            message=f'"poems": "{truncated_poem}"',
            system_message=(
                "You are Qwen, created by Alibaba Cloud. You are a helpful assistant that will give 3 Persian noun "
                "keywords without explanation that represent the poem in Python list format."
            ),
            max_tokens=50,
            temperature=0.7,
            top_p=0.8,
            api_name="/chat"
        )
        # Extract keywords from response
        keywords = response.strip("[]").replace("'", "").split(", ")
        return keywords[:3]  # Ensure only 5 keywords
    except Exception as e:
        print(f"Error processing poem: {e}")
        return []

# Process the data
counter = 1725  # Initialize counter

for entry in data[1725:2050]:
    poem_text = entry.get("poem", "")  # Extract the "poem" field
    if poem_text:
        # Get keywords for the poem
        keywords = get_keywords(poem_text)
        entry["keywords"] = keywords  # Add the keywords to the entry
        print(f"Processed poem {counter}: {keywords}")
        counter += 1

# Print updated data
# print(json.dumps(data, ensure_ascii=False, indent=4))

data[2050]["keywords"]

import json
from typing import List
from gradio_client import Client

# Initialize the client
client = Client("llamameta/Qwen2.5-Coder-32B-Instruct-Chat-Assistant")

# Function to truncate text based on token approximation
def truncate_text(text: str, max_tokens: int = 32000) -> str:
    # Truncate based on character length as a rough approximation
    return text[:max_tokens]

# Function to get keywords from a poem
def get_keywords(poem: str) -> List[str]:
    truncated_poem = truncate_text(poem, max_tokens=32000)  # Adjust token space if needed
    try:
        response = client.predict(
            message=f'"poems": "{truncated_poem}"',
            system_message=(
                "You are Qwen, created by Alibaba Cloud. You are a helpful assistant that will give 3 Persian noun "
                "keywords without explanation that represent the poem in Python list format."
            ),
            max_tokens=50,
            temperature=0.7,
            top_p=0.8,
            api_name="/chat"
        )
        # Extract keywords from response
        keywords = response.strip("[]").replace("'", "").split(", ")
        return keywords[:3]  # Ensure only 5 keywords
    except Exception as e:
        print(f"Error processing poem: {e}")
        return []

# Process the data
# counter = 1  # Initialize counter

for entry in data[2050:2400]:
    poem_text = entry.get("poem", "")  # Extract the "poem" field
    if poem_text:
        # Get keywords for the poem
        keywords = get_keywords(poem_text)
        entry["keywords"] = keywords  # Add the keywords to the entry
        print(f"Processed poem {counter}: {keywords}")
        counter += 1

# Print updated data
# print(json.dumps(data, ensure_ascii=False, indent=4))

import json
from typing import List
from gradio_client import Client

# Initialize the client
client = Client("llamameta/Qwen2.5-Coder-32B-Instruct-Chat-Assistant")

# Function to truncate text based on token approximation
def truncate_text(text: str, max_tokens: int = 32000) -> str:
    # Truncate based on character length as a rough approximation
    return text[:max_tokens]

# Function to get keywords from a poem
def get_keywords(poem: str) -> List[str]:
    truncated_poem = truncate_text(poem, max_tokens=32000)  # Adjust token space if needed
    try:
        response = client.predict(
            message=f'"poems": "{truncated_poem}"',
            system_message=(
                "You are Qwen, created by Alibaba Cloud. You are a helpful assistant that will give 3 Persian noun "
                "keywords without explanation that represent the poem in Python list format."
            ),
            max_tokens=50,
            temperature=0.7,
            top_p=0.8,
            api_name="/chat"
        )
        # Extract keywords from response
        keywords = response.strip("[]").replace("'", "").split(", ")
        return keywords[:3]  # Ensure only 5 keywords
    except Exception as e:
        print(f"Error processing poem: {e}")
        return []

# Process the data
# counter = 1  # Initialize counter

for entry in data[2400:2750]:
    poem_text = entry.get("poem", "")  # Extract the "poem" field
    if poem_text:
        # Get keywords for the poem
        keywords = get_keywords(poem_text)
        entry["keywords"] = keywords  # Add the keywords to the entry
        print(f"Processed poem {counter}: {keywords}")
        counter += 1

# Print updated data
# print(json.dumps(data, ensure_ascii=False, indent=4))

data[2737]

import json
from typing import List
from gradio_client import Client

# Initialize the client
client = Client("llamameta/Qwen2.5-Coder-32B-Instruct-Chat-Assistant")

# Function to truncate text based on token approximation
def truncate_text(text: str, max_tokens: int = 32000) -> str:
    # Truncate based on character length as a rough approximation
    return text[:max_tokens]

# Function to get keywords from a poem
def get_keywords(poem: str) -> List[str]:
    truncated_poem = truncate_text(poem, max_tokens=32000)  # Adjust token space if needed
    try:
        response = client.predict(
            message=f'"poems": "{truncated_poem}"',
            system_message=(
                "You are Qwen, created by Alibaba Cloud. You are a helpful assistant that will give 3 Persian noun"
                "keywords without explanation that represent the poem in Python list format."
            ),
            max_tokens=50,
            temperature=0.7,
            top_p=0.8,
            api_name="/chat"
        )
        # Extract keywords from response
        keywords = response.strip("[]").replace("'", "").split(", ")
        return keywords[:3]  # Ensure only 5 keywords
    except Exception as e:
        print(f"Error processing poem: {e}")
        return []

# Process the data
counter = 2738  # Initialize counter

for entry in data[2738:3050]:
    poem_text = entry.get("poem", "")  # Extract the "poem" field
    if poem_text:
        # Get keywords for the poem
        keywords = get_keywords(poem_text)
        entry["keywords"] = keywords  # Add the keywords to the entry
        print(f"Processed poem {counter}: {keywords}")
        counter += 1

# Print updated data
# print(json.dumps(data, ensure_ascii=False, indent=4))

import json
from typing import List
from gradio_client import Client

# Initialize the client
client = Client("llamameta/Qwen2.5-Coder-32B-Instruct-Chat-Assistant")

# Function to truncate text based on token approximation
def truncate_text(text: str, max_tokens: int = 32000) -> str:
    # Truncate based on character length as a rough approximation
    return text[:max_tokens]

# Function to get keywords from a poem
def get_keywords(poem: str) -> List[str]:
    truncated_poem = truncate_text(poem, max_tokens=32000)  # Adjust token space if needed
    try:
        response = client.predict(
            message=f'"poems": "{truncated_poem}"',
            system_message=(
                "You are Qwen, created by Alibaba Cloud. You are a helpful assistant that will give 3 Persian noun "
                "keywords without explanation that represent the poem in Python list format."
            ),
            max_tokens=50,
            temperature=0.7,
            top_p=0.8,
            api_name="/chat"
        )
        # Extract keywords from response
        keywords = response.strip("[]").replace("'", "").split(", ")
        return keywords[:3]  # Ensure only 5 keywords
    except Exception as e:
        print(f"Error processing poem: {e}")
        return []

# Process the data
counter = 2987  # Initialize counter

for entry in data[2987:3250]:
    poem_text = entry.get("poem", "")  # Extract the "poem" field
    if poem_text:
        # Get keywords for the poem
        keywords = get_keywords(poem_text)
        entry["keywords"] = keywords  # Add the keywords to the entry
        print(f"Processed poem {counter}: {keywords}")
        counter += 1

# Print updated data
# print(json.dumps(data, ensure_ascii=False, indent=4))

data[3249]

import json
from typing import List
from gradio_client import Client

# Initialize the client
client = Client("llamameta/Qwen2.5-Coder-32B-Instruct-Chat-Assistant")

# Function to truncate text based on token approximation
def truncate_text(text: str, max_tokens: int = 32000) -> str:
    # Truncate based on character length as a rough approximation
    return text[:max_tokens]

# Function to get keywords from a poem
def get_keywords(poem: str) -> List[str]:
    truncated_poem = truncate_text(poem, max_tokens=32000)  # Adjust token space if needed
    try:
        response = client.predict(
            message=f'"poems": "{truncated_poem}"',
            system_message=(
                "You are Qwen, created by Alibaba Cloud. You are a helpful assistant that will give 3 Persian noun"
                "keywords without explanation that represent the poem in Python list format."
            ),
            max_tokens=50,
            temperature=0.7,
            top_p=0.8,
            api_name="/chat"
        )
        # Extract keywords from response
        keywords = response.strip("[]").replace("'", "").split(", ")
        return keywords[:3]  # Ensure only 5 keywords
    except Exception as e:
        print(f"Error processing poem: {e}")
        return []

# Process the data
# counter = 1  # Initialize counter

for entry in data[3250:3550]:
    poem_text = entry.get("poem", "")  # Extract the "poem" field
    if poem_text:
        # Get keywords for the poem
        keywords = get_keywords(poem_text)
        entry["keywords"] = keywords  # Add the keywords to the entry
        print(f"Processed poem {counter}: {keywords}")
        counter += 1

# Print updated data
# print(json.dumps(data, ensure_ascii=False, indent=4))

import json
from typing import List
from gradio_client import Client

# Initialize the client
client = Client("llamameta/Qwen2.5-Coder-32B-Instruct-Chat-Assistant")

# Function to truncate text based on token approximation
def truncate_text(text: str, max_tokens: int = 32000) -> str:
    # Truncate based on character length as a rough approximation
    return text[:max_tokens]

# Function to get keywords from a poem
def get_keywords(poem: str) -> List[str]:
    truncated_poem = truncate_text(poem, max_tokens=32000)  # Adjust token space if needed
    try:
        response = client.predict(
            message=f'"poems": "{truncated_poem}"',
            system_message=(
                "You are Qwen, created by Alibaba Cloud. You are a helpful assistant that will give 3 Persian noun"
                "keywords without explanation that represent the poem in Python list format."
            ),
            max_tokens=50,
            temperature=0.7,
            top_p=0.8,
            api_name="/chat"
        )
        # Extract keywords from response
        keywords = response.strip("[]").replace("'", "").split(", ")
        return keywords[:3]  # Ensure only 5 keywords
    except Exception as e:
        print(f"Error processing poem: {e}")
        return []

# Process the data
# counter = 1  # Initialize counter

for entry in data[3550:3850]:
    poem_text = entry.get("poem", "")  # Extract the "poem" field
    if poem_text:
        # Get keywords for the poem
        keywords = get_keywords(poem_text)
        entry["keywords"] = keywords  # Add the keywords to the entry
        print(f"Processed poem {counter}: {keywords}")
        counter += 1

# Print updated data
# print(json.dumps(data, ensure_ascii=False, indent=4))

import json
from typing import List
from gradio_client import Client

# Initialize the client
client = Client("llamameta/Qwen2.5-Coder-32B-Instruct-Chat-Assistant")

# Function to truncate text based on token approximation
def truncate_text(text: str, max_tokens: int = 32000) -> str:
    # Truncate based on character length as a rough approximation
    return text[:max_tokens]

# Function to get keywords from a poem
def get_keywords(poem: str) -> List[str]:
    truncated_poem = truncate_text(poem, max_tokens=32000)  # Adjust token space if needed
    try:
        response = client.predict(
            message=f'"poems": "{truncated_poem}"',
            system_message=(
                "You are Qwen, created by Alibaba Cloud. You are a helpful assistant that will give 3 Persian noun"
                "keywords without explanation that represent the poem in Python list format."
            ),
            max_tokens=50,
            temperature=0.7,
            top_p=0.8,
            api_name="/chat"
        )
        # Extract keywords from response
        keywords = response.strip("[]").replace("'", "").split(", ")
        return keywords[:3]  # Ensure only 5 keywords
    except Exception as e:
        print(f"Error processing poem: {e}")
        return []

# Process the data
counter = 3850  # Initialize counter

for entry in data[3850:]:
    poem_text = entry.get("poem", "")  # Extract the "poem" field
    if poem_text:
        # Get keywords for the poem
        keywords = get_keywords(poem_text)
        entry["keywords"] = keywords  # Add the keywords to the entry
        print(f"Processed poem {counter}: {keywords}")
        counter += 1

# Print updated data
# print(json.dumps(data, ensure_ascii=False, indent=4))
output_file = "Hafez_verse_keywords.json"
with open(output_file, "w", encoding="utf-8") as f:
    json.dump(data, f, ensure_ascii=False, indent=4)

print(f"Data saved to {output_file}")



"""#encoder decoder

"""

import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import DataLoader, Dataset
from sklearn.model_selection import train_test_split
from torch.nn.utils.rnn import pad_sequence
from collections import Counter
import numpy as np

# 1. Data Preparation
class PersianDataset(Dataset):
    def __init__(self, texts, tokenizer, max_length):
        self.texts = texts
        self.tokenizer = tokenizer
        self.max_length = max_length
        self.data = [self._encode(text) for text in texts]

    def _encode(self, text):
        tokens = self.tokenizer(text)
        return torch.tensor(tokens, dtype=torch.long)

    def __len__(self):
        return len(self.data)

    def __getitem__(self, idx):
        seq = self.data[idx]
        return seq, seq  # Input and target are the same

def tokenize(texts, vocab=None):
    # Simple space-based tokenizer
    tokenized = [text.split() for text in texts]
    if vocab is None:
        vocab = Counter(word for line in tokenized for word in line)
    word_to_idx = {word: idx + 1 for idx, (word, _) in enumerate(vocab.items())}  # Reserve 0 for padding
    idx_to_word = {idx: word for word, idx in word_to_idx.items()}
    sequences = [[word_to_idx[word] for word in line] for line in tokenized]
    return sequences, word_to_idx, idx_to_word

# Sample Persian text
verses = [
    "به نام خداوند جان و خرد",
    "کزین برتر اندیشه برنگذرد",
    "خداوند کیوان و گردون سپهر",
    "فروزنده ماه و ناهید و مهر"
]

# Build vocabulary and tokenize
sequences, word_to_idx, idx_to_word = tokenize(verses)
vocab_size = len(word_to_idx) + 1  # +1 for padding
max_length = max(len(seq) for seq in sequences)

# Dataset and DataLoader
dataset = PersianDataset(verses, lambda x: [word_to_idx[word] for word in x.split()], max_length)
dataloader = DataLoader(dataset, batch_size=2, shuffle=True, collate_fn=lambda x: pad_sequence([s[0] for s in x], batch_first=True, padding_value=0))

# 2. Define the Model
class Autoencoder(nn.Module):
    def __init__(self, vocab_size, embedding_dim, hidden_dim):
        super(Autoencoder, self).__init__()
        self.embedding = nn.Embedding(vocab_size, embedding_dim, padding_idx=0)
        self.encoder = nn.LSTM(embedding_dim, hidden_dim, batch_first=True)
        self.decoder = nn.LSTM(embedding_dim, hidden_dim, batch_first=True)
        self.output_layer = nn.Linear(hidden_dim, vocab_size)

    def forward(self, x):
        # Embedding
        embedded = self.embedding(x)
        # Encode
        _, (hidden, cell) = self.encoder(embedded)
        # Decode
        decoder_output, _ = self.decoder(embedded, (hidden, cell))
        # Output layer
        output = self.output_layer(decoder_output)
        return output

# Model Parameters
embedding_dim = 128
hidden_dim = 256
model = Autoencoder(vocab_size, embedding_dim, hidden_dim)

# Loss and Optimizer
criterion = nn.CrossEntropyLoss(ignore_index=0)
optimizer = optim.Adam(model.parameters(), lr=0.001)

# 3. Train the Model
epochs = 20
for epoch in range(epochs):
    model.train()
    total_loss = 0
    for batch in dataloader:
        inputs, targets = batch, batch
        optimizer.zero_grad()
        outputs = model(inputs)

        # Reshape outputs and targets for loss calculation
        outputs = outputs.view(-1, vocab_size)
        targets = targets.view(-1)

        loss = criterion(outputs, targets)
        loss.backward()
        optimizer.step()
        total_loss += loss.item()

    print(f"Epoch {epoch + 1}, Loss: {total_loss / len(dataloader):.4f}")

# Save Model
torch.save(model.state_dict(), "persian_verse_autoencoder.pth")

# 4. Inference
def generate(model, input_seq):
    model.eval()
    with torch.no_grad():
        input_seq = torch.tensor(input_seq, dtype=torch.long).unsqueeze(0)  # Add batch dimension
        output = model(input_seq)
        output_indices = torch.argmax(output, dim=-1).squeeze(0).tolist()
    return [idx_to_word[idx] for idx in output_indices if idx != 0]

# Example Inference
sample_input = [word_to_idx[word] for word in "به نام خداوند".split()]
print(" ".join(generate(model, sample_input)))

